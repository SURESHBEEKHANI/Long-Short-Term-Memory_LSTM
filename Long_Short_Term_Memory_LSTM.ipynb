{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwvYMBh7ER2bpK1MU32gu7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Long-Short-Term-Memory_LSTM/blob/main/Long_Short_Term_Memory_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsWSLW3GlwIY"
      },
      "outputs": [],
      "source": [
        "# Import the numpy library, which helps with mathematical operations\n",
        "import numpy as np\n",
        "\n",
        "# Import the tensorflow library, which is used for building and training machine learning models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import the Sequential class from tensorflow, which helps in building a sequence of layers for our model\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Import the Tokenizer class from tensorflow, which helps to process and prepare text data for our model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Import the layers we will use in our model: Dense (fully connected layers), Embedding (for converting words into numerical data), and LSTM (a type of layer useful for understanding sequences)\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "\n",
        "# Import the pad_sequences function from tensorflow, which helps to make sure all sequences (like sentences) are the same length by adding padding if needed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Fixed a typo here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data"
      ],
      "metadata": {
        "id": "hLVnMTSs_kTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "id": "wkkiC0m3O1IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the PdfReader class from the PyPDF2 library, which helps us read PDF files\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Open the PDF file in read-binary mode ('rb') so we can work with it\n",
        "with open(\"/content/thebook.pdf\", \"rb\") as file:\n",
        "    # Create a PdfReader object to read the PDF file\n",
        "    reader = PdfReader(file)\n",
        "\n",
        "    # Create an empty list to store the text from each page of the PDF\n",
        "    list_text = []\n",
        "\n",
        "    # Loop through each page in the PDF file\n",
        "    for page_num in range(len(reader.pages)):\n",
        "        # Get the current page using its number\n",
        "        page = reader.pages[page_num]\n",
        "\n",
        "        # Extract the text from the current page\n",
        "        text = page.extract_text()\n",
        "\n",
        "        # Add the extracted text to the list\n",
        "        list_text.append(text)\n",
        "\n",
        "# Print the list that contains the text from all pages\n",
        "print(list_text)\n"
      ],
      "metadata": {
        "id": "yzCX12Lt5uxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization and Preprocessing data\n",
        "\n",
        "Tokenization is the process of breaking text into smaller units (words, sentences, etc.). Preprocessing involves cleaning and normalizing text by steps such as lowercasing, removing punctuation and stop words, and stemming or lemmatizing words. Together, these steps prepare text data for analysis or machine learning."
      ],
      "metadata": {
        "id": "rWUM4bFh_oGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the Tokenizer object to preprocess and vectorize text data\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the Tokenizer on the list of sentences (or texts) to build the vocabulary\n",
        "tokenizer.fit_on_texts(list_text)\n",
        "\n",
        "# Get the total number of unique words in the vocabulary, and assign and index value\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Print the total number of unique words in the vocabulary\n",
        "print(total_words)\n",
        "\n",
        "# Print the word index dictionary that maps words to their integer index\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "\n",
        "# Initialize an empty list to hold the input sequences\n",
        "input_sequence = []\n",
        "\n",
        "# Iterate through each sentence in the list of sentences\n",
        "for line in list_text:\n",
        "    # Convert each sentence into a sequence of integers using the tokenizer\n",
        "    # The 'texts_to_sequences' method converts words to their corresponding integer indices\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list) + 1):  # Use len(token_list) + 1 to include the last element\n",
        "        n_gram_sequence = token_list[:i]\n",
        "        input_sequence.append(n_gram_sequence)\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_sequence_len = max(len(seq) for seq in input_sequence)\n",
        "\n",
        "# Pad sequences to\n",
        "padded_sequences = pad_sequences(input_sequence, maxlen=max_sequence_len, padding='pre')\n",
        "print(padded_sequences)\n",
        "\n",
        "# Summary:\n",
        "# This code initializes a Tokenizer, fits it on a list of sentences to build the vocabulary,\n",
        "# and then prints the total number of unique words and their corresponding indices in the word index."
      ],
      "metadata": {
        "id": "_D_f5mhN-20R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the data into features (x) and targets (y)"
      ],
      "metadata": {
        "id": "mi3v4x6IMaLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (x) and targets (y)\n",
        "# Features (x) include all columns except the last one\n",
        "x_train = padded_sequences[:, :-1]\n",
        "\n",
        "# Targets (y) include only the last column\n",
        "y_train = padded_sequences[:, -1]\n",
        "\n",
        "# Print the features to verify their content\n",
        "print(\"Features (x):\")\n",
        "print(x_train)\n",
        "\n",
        "# Print the targets to verify their content\n",
        "print(\"Targets (y):\")\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "awoKRkeOKljs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert the target data (y) to one-hot encoded format"
      ],
      "metadata": {
        "id": "rtK07Ie-MT8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert the target data (y) to one-hot encoded format\n",
        "# 'num_classes' specifies the total number of classes (e.g., total_words)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=total_words)\n",
        "\n",
        "# Print the one-hot encoded target data to verify the result\n",
        "print(\"One-hot encoded targets (y):\")\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "NzESdygFKwiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Architecture RNN (Recurrent Neural Network)"
      ],
      "metadata": {
        "id": "VFln_XRhMKe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    # Embedding layer: converts input sequences of integers into dense vectors of fixed size\n",
        "    Embedding(input_dim=total_words,  # Size of the vocabulary\n",
        "              output_dim=10,           # Dimension of the dense embedding vectors\n",
        "              input_length=max_sequence_len-1),  # Length of input sequences\n",
        "\n",
        "    # Simple RNN layer: processes the sequence data and retains the temporal information\n",
        "    LSTM(100),  # Number of units in the RNN layer\n",
        "\n",
        "    # Dense layer: outputs the probability distribution over the vocabulary\n",
        "    Dense(total_words,            # Size of the output layer (same as vocabulary size)\n",
        "          activation='softmax')   # Softmax activation to produce probability distribution\n",
        "])\n"
      ],
      "metadata": {
        "id": "7q5Epb1pLlB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "tMpihk36M8Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train,                    # Input data\n",
        "    y_train,                    # Target labels\n",
        "    epochs=50,                  # Number of training epochs              # Number of samples per gradient update\n",
        "    validation_split=0.2,       # Fraction of data to be used for validation\n",
        "    verbose=1                   # Verbosity mode: 1 for progress bar\n",
        ")"
      ],
      "metadata": {
        "id": "6DAZJH3xNLZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This function predicts the next word(s) given a starting piece of text (seed_text).\n",
        "def predict_next_word(seed_text, next_words=1):\n",
        "    # Loop to predict the next word 'next_words' times\n",
        "    for _ in range(next_words):\n",
        "        # Convert the seed text into a format that the model can understand (a sequence of numbers)\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])\n",
        "\n",
        "        # Ensure the sequence is the right length for the model by padding it with zeros if needed\n",
        "        token_list = pad_sequences(token_list, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "        # Use the model to predict the probabilities of the next word\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        # Find the index of the word with the highest probability\n",
        "        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n",
        "\n",
        "        # Convert the index back into the actual word\n",
        "        predicted_word = tokenizer.index_word.get(predicted_index, '')\n",
        "\n",
        "        # Add the predicted word to the end of the seed text\n",
        "        seed_text += ' ' + predicted_word\n",
        "\n",
        "    # Return the updated seed text with the predicted words added\n",
        "    return seed_text\n",
        "\n",
        "# Example usage of the function:\n",
        "seed_text = \"what is machine learining  \"\n",
        "# Predict the next 4 words based on the seed text\n",
        "predicted_text = predict_next_word(seed_text, next_words=50)\n",
        "# Print the result\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "id": "Y9SUT2woOAgj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}