{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNexJBNx3vI3C5X9O/VMTHh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Long-Short-Term-Memory_LSTM/blob/main/Long_Short_Term_Memory_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsWSLW3GlwIY"
      },
      "outputs": [],
      "source": [
        "# Import the numpy library, which helps with mathematical operations\n",
        "import numpy as np\n",
        "\n",
        "# Import the tensorflow library, which is used for building and training machine learning models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import the Sequential class from tensorflow, which helps in building a sequence of layers for our model\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Import the Tokenizer class from tensorflow, which helps to process and prepare text data for our model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Import the layers we will use in our model: Dense (fully connected layers), Embedding (for converting words into numerical data), and LSTM (a type of layer useful for understanding sequences)\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "\n",
        "# Import the pad_sequences function from tensorflow, which helps to make sure all sequences (like sentences) are the same length by adding padding if needed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Fixed a typo here\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2\n"
      ],
      "metadata": {
        "id": "9VuRqfod3vX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data"
      ],
      "metadata": {
        "id": "hLVnMTSs_kTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the PdfReader class from the PyPDF2 library, which helps us read PDF files\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Open the PDF file in read-binary mode ('rb') so we can work with it\n",
        "with open(\"/content/thebook.pdf\", \"rb\") as file:\n",
        "    # Create a PdfReader object to read the PDF file\n",
        "    reader = PdfReader(file)\n",
        "\n",
        "    # Create an empty list to store the text from each page of the PDF\n",
        "    list_text = []\n",
        "\n",
        "    # Loop through each page in the PDF file\n",
        "    for page_num in range(len(reader.pages)):\n",
        "        # Get the current page using its number\n",
        "        page = reader.pages[page_num]\n",
        "\n",
        "        # Extract the text from the current page\n",
        "        text = page.extract_text()\n",
        "\n",
        "        # Add the extracted text to the list\n",
        "        list_text.append(text)\n",
        "\n",
        "# Print the list that contains the text from all pages\n",
        "print(list_text)\n"
      ],
      "metadata": {
        "id": "yzCX12Lt5uxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and Preprocessing data\n",
        "\n",
        "Tokenization is the process of breaking text into smaller units (words, sentences, etc.). Preprocessing involves cleaning and normalizing text by steps such as lowercasing, removing punctuation and stop words, and stemming or lemmatizing words. Together, these steps prepare text data for analysis or machine learning."
      ],
      "metadata": {
        "id": "rWUM4bFh_oGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the Tokenizer object to preprocess and vectorize text data\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the Tokenizer on the list of sentences (or texts) to build the vocabulary\n",
        "tokenizer.fit_on_texts(list_text)\n",
        "\n",
        "# Get the total number of unique words in the vocabulary, and assign and index value\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Print the total number of unique words in the vocabulary\n",
        "print(total_words)\n",
        "\n",
        "# Print the word index dictionary that maps words to their integer index\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "\n",
        "# Initialize an empty list to hold the input sequences\n",
        "input_sequence = []\n",
        "\n",
        "# Iterate through each sentence in the list of sentences\n",
        "for line in list_text:\n",
        "    # Convert each sentence into a sequence of integers using the tokenizer\n",
        "    # The 'texts_to_sequences' method converts words to their corresponding integer indices\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list) + 1):  # Use len(token_list) + 1 to include the last element\n",
        "        n_gram_sequence = token_list[:i]\n",
        "        input_sequence.append(n_gram_sequence)\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_sequence_len = max(len(seq) for seq in input_sequence)\n",
        "\n",
        "# Pad sequences to\n",
        "padded_sequences = pad_sequences(input_sequence, maxlen=max_sequence_len, padding='pre')\n",
        "print(padded_sequences)\n",
        "\n",
        "# Summary:\n",
        "# This code initializes a Tokenizer, fits it on a list of sentences to build the vocabulary,\n",
        "# and then prints the total number of unique words and their corresponding indices in the word index."
      ],
      "metadata": {
        "id": "_D_f5mhN-20R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (x) and targets (y)\n",
        "# Features (x) include all columns except the last one\n",
        "x_train = padded_sequences[:, :-1]\n",
        "\n",
        "# Targets (y) include only the last column\n",
        "y_train = padded_sequences[:, -1]\n",
        "\n",
        "# Print the features to verify their content\n",
        "print(\"Features (x):\")\n",
        "print(x_train)\n",
        "\n",
        "# Print the targets to verify their content\n",
        "print(\"Targets (y):\")\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "awoKRkeOKljs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert the target data (y) to one-hot encoded format\n",
        "# 'num_classes' specifies the total number of classes (e.g., total_words)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=total_words)\n",
        "\n",
        "# Print the one-hot encoded target data to verify the result\n",
        "print(\"One-hot encoded targets (y):\")\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "NzESdygFKwiD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}